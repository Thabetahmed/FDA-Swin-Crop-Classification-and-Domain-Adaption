{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed50c0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: Imports and Setup\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import timm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c9ba529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: Configuration\n",
    "# =============================================================================\n",
    "\n",
    "# Paths (Adjust as needed)\n",
    "PATH_PASTIS = Path('PASTIS/DATA_S2')  # Raw PASTIS data\n",
    "PATH_ALGERIA_CEREAL = Path('output/algeria_s2_data/DATA_S2')\n",
    "PATH_ALGERIA_ANN = Path('output/algeria_s2_data/ANNOTATIONS')\n",
    "PATH_ALGERIA_POTATO = Path('output/potato_inference_data')\n",
    "\n",
    "CONFIG = {\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 1e-5,\n",
    "    'weight_decay': 0.05,\n",
    "    'epochs': 20,\n",
    "    'img_size': 128,\n",
    "    'in_channels': 10,\n",
    "    'fda_beta': 0.05, # Strength of domain adaptation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ad88468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: FDA (Fourier Domain Adaptation) Function\n",
    "# =============================================================================\n",
    "\n",
    "def extract_spectrum(img_np):\n",
    "    \"\"\"Computes FFT of the image (amplitude and phase)\"\"\"\n",
    "    # img_np shape: (C, H, W)\n",
    "    fft = np.fft.fft2(img_np, axes=(-2, -1))\n",
    "    amplitude = np.abs(fft)\n",
    "    phase = np.angle(fft)\n",
    "    return amplitude, phase\n",
    "\n",
    "def apply_fda(source_img, target_img, beta=0.01):\n",
    "    \"\"\"\n",
    "    Transfers the low-frequency style (amplitude) from target to source.\n",
    "    \"\"\"\n",
    "    # 1. Get FFT of both\n",
    "    src_amp, src_phase = extract_spectrum(source_img)\n",
    "    trg_amp, _ = extract_spectrum(target_img)\n",
    "    \n",
    "    # 2. Define low-frequency window\n",
    "    c, h, w = source_img.shape\n",
    "    b = int(np.floor(min(h, w) * beta))\n",
    "    center_h, center_w = int(h/2), int(w/2)\n",
    "    \n",
    "    # 3. Swap low-freq amplitude (Style Transfer)\n",
    "    # Shift FFT so low freq is in center\n",
    "    src_amp_shifted = np.fft.fftshift(src_amp, axes=(-2, -1))\n",
    "    trg_amp_shifted = np.fft.fftshift(trg_amp, axes=(-2, -1))\n",
    "    \n",
    "    # Replace center (low freq) of source with target\n",
    "    src_amp_shifted[:, center_h-b:center_h+b, center_w-b:center_w+b] = \\\n",
    "        trg_amp_shifted[:, center_h-b:center_h+b, center_w-b:center_w+b]\n",
    "    \n",
    "    # Shift back\n",
    "    new_amp = np.fft.ifftshift(src_amp_shifted, axes=(-2, -1))\n",
    "    \n",
    "    # 4. Reconstruct\n",
    "    new_fft = new_amp * np.exp(1j * src_phase)\n",
    "    new_img = np.abs(np.fft.ifft2(new_fft, axes=(-2, -1)))\n",
    "    \n",
    "    return np.clip(new_img, 0, 1).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c033e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_swin_binary(in_channels=10, num_classes=1, pretrained=False, img_size=128):\n",
    "    model = timm.create_model(\n",
    "        'swin_tiny_patch4_window7_224', # Use 128 size version if available, else 224 and resize\n",
    "        pretrained=pretrained,\n",
    "        num_classes=num_classes,\n",
    "        img_size=img_size,\n",
    "    )\n",
    "    \n",
    "    # Patch Embedding Hack for 10 Channels\n",
    "    original_proj = model.patch_embed.proj\n",
    "    new_proj = nn.Conv2d(\n",
    "        in_channels=CONFIG['in_channels'],\n",
    "        out_channels=original_proj.out_channels,\n",
    "        kernel_size=original_proj.kernel_size,\n",
    "        stride=original_proj.stride,\n",
    "        padding=original_proj.padding\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        new_proj.weight[:, :3, :, :] = original_proj.weight\n",
    "        mean_weight = torch.mean(original_proj.weight, dim=1, keepdim=True)\n",
    "        new_proj.weight[:, 3:, :, :] = mean_weight.repeat(1, 7, 1, 1)\n",
    "        new_proj.bias.copy_(original_proj.bias)\n",
    "        \n",
    "    model.patch_embed.proj = new_proj\n",
    "    return model\n",
    "\n",
    "model = create_swin_binary().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c9f160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: Dataset Classes\n",
    "# =============================================================================\n",
    "\n",
    "# --- 1. PASTIS DATASET (The \"Teacher\" Data - FDA Adapted) ---\n",
    "class PastisDataset(Dataset):\n",
    "    def __init__(self, data_dir, target_style_images=None):\n",
    "        self.files = sorted(list(data_dir.glob('*.npy')))\n",
    "        self.target_style_images = target_style_images # List of Algeria images for FDA\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.files[idx]\n",
    "        try:\n",
    "            full_ts = np.load(path)\n",
    "            if full_ts.ndim == 4:\n",
    "                # Simple Max-NDVI selection on the fly\n",
    "                # NDVI = (B8 - B4) / (B8 + B4) -> B8 is index 6, B4 is index 2\n",
    "                b8 = full_ts[:, 6, :, :]\n",
    "                b4 = full_ts[:, 2, :, :]\n",
    "                ndvi = (b8 - b4) / (b8 + b4 + 1e-6)\n",
    "                mean_ndvi = ndvi.mean(axis=(1, 2))\n",
    "                best_idx = np.argmax(mean_ndvi)\n",
    "                img = full_ts[best_idx].astype(np.float32)\n",
    "            else:\n",
    "                img = full_ts.astype(np.float32)\n",
    "            \n",
    "            img = img / 10000.0\n",
    "            img = np.clip(img, 0, 1)\n",
    "\n",
    "            # --- APPLY FDA (Domain Adaptation) ---\n",
    "            if self.target_style_images is not None and len(self.target_style_images) > 0:\n",
    "                target_idx = random.randint(0, len(self.target_style_images)-1)\n",
    "                target_img = self.target_style_images[target_idx]\n",
    "                img = apply_fda(img, target_img, beta=CONFIG['fda_beta'])\n",
    "            \n",
    "            # Label 1 (We treat PASTIS crops as Positive training examples)\n",
    "            return torch.from_numpy(img), torch.tensor(1.0, dtype=torch.float32)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return torch.zeros((10, 128, 128)), torch.tensor(0.0)\n",
    "\n",
    "# --- 2. ALGERIA MIXED DATASET (Cereal + Potato) ---\n",
    "class AlgeriaMixedDataset(Dataset):\n",
    "    def __init__(self, cereal_files, potato_files, cereal_ann_dir):\n",
    "        self.files = cereal_files + potato_files\n",
    "        self.cereal_ann_dir = cereal_ann_dir\n",
    "        self.potato_files_set = set(potato_files) # For fast lookup\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.files[idx]\n",
    "        img = np.load(path).astype(np.float32)\n",
    "        if img.ndim == 4: img = img[0]\n",
    "        img = img / 10000.0\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        # --- DETERMINE LABEL (BINARY: Crop vs No-Crop) ---\n",
    "        label_val = 0.0\n",
    "        \n",
    "        if path in self.potato_files_set:\n",
    "            # Logic for Potato Files: _P1_ is Potato (Crop), _P0_ is Non-Potato (Background)\n",
    "            if '_P1_' in path.name:\n",
    "                label_val = 1.0\n",
    "            else:\n",
    "                label_val = 0.0\n",
    "        else:\n",
    "            # Logic for Cereal Files (Check Annotation)\n",
    "            fid = path.stem.replace('S2_', '')\n",
    "            ann_path = self.cereal_ann_dir / f\"Labels_{fid}.npy\"\n",
    "            if ann_path.exists():\n",
    "                ann = np.load(ann_path)\n",
    "                # If mask has any crop class (1, 2, 3..), it's Crop. 0 is Background.\n",
    "                max_class = ann.max()\n",
    "                if max_class > 0:\n",
    "                    label_val = 1.0\n",
    "                else:\n",
    "                    label_val = 0.0\n",
    "            else:\n",
    "                label_val = 0.0 # Default background\n",
    "        \n",
    "        return torch.from_numpy(img), torch.tensor(label_val, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70c70a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cereal: 100 Train, 93 Test\n",
      "Potato: 42 Train, 43 Test\n",
      "Loading style references for FDA...\n",
      "\n",
      "FINAL DATA SETUP:\n",
      "  Training: ~4928 samples (Hybrid)\n",
      "  Testing:  136 samples (Algeria Only)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: Data Loading & Splitting\n",
    "# =============================================================================\n",
    "\n",
    "# 1. Gather Files\n",
    "cereal_files = sorted(list(PATH_ALGERIA_CEREAL.glob('*.npy')))\n",
    "potato_files = sorted(list(PATH_ALGERIA_POTATO.glob('S2_*.npy')))\n",
    "\n",
    "# 2. Split Algeria Data (Train vs Test)\n",
    "# Cereal Split (Standard < 100 logic from previous notebook)\n",
    "cereal_train = [f for f in cereal_files if int(f.stem.split('_')[1]) <= 100]\n",
    "cereal_test = [f for f in cereal_files if int(f.stem.split('_')[1]) > 100]\n",
    "\n",
    "# Potato Split (Random Tile Split - No Poly Logic as requested)\n",
    "pot_train, pot_test = train_test_split(potato_files, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "print(f\"Cereal: {len(cereal_train)} Train, {len(cereal_test)} Test\")\n",
    "print(f\"Potato: {len(pot_train)} Train, {len(pot_test)} Test\")\n",
    "# 3. Load Target Images for FDA (Use Algeria Train images as style targets)\n",
    "style_refs = []\n",
    "print(\"Loading style references for FDA...\")\n",
    "for f in (cereal_train + pot_train)[:50]: # Take 50 random samples\n",
    "    img = np.load(f).astype(np.float32)\n",
    "    if img.ndim == 4: img = img[0]\n",
    "    img = img / 10000.0\n",
    "    style_refs.append(np.clip(img, 0, 1))\n",
    "\n",
    "# 4. Instantiate Datasets\n",
    "ds_pastis = PastisDataset(PATH_PASTIS, target_style_images=style_refs)\n",
    "ds_algeria_train = AlgeriaMixedDataset(cereal_train, pot_train, PATH_ALGERIA_ANN)\n",
    "ds_algeria_test = AlgeriaMixedDataset(cereal_test, pot_test, PATH_ALGERIA_ANN)\n",
    "\n",
    "# 5. Hybrid DataLoader (50% FDA, 50% Real)\n",
    "class HybridDataLoader:\n",
    "    def __init__(self, fda_dataset, real_dataset, batch_size=32, shuffle=True):\n",
    "        self.fda_dataset = fda_dataset\n",
    "        self.real_dataset = real_dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.fda_per_batch = batch_size // 2\n",
    "        self.real_per_batch = batch_size - self.fda_per_batch\n",
    "        \n",
    "        self.fda_indices = list(range(len(fda_dataset)))\n",
    "        self.real_indices = list(range(len(real_dataset)))\n",
    "        self.num_batches = len(fda_dataset) // self.fda_per_batch # Oversample real to match FDA\n",
    "        \n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.fda_indices)\n",
    "            random.shuffle(self.real_indices)\n",
    "        \n",
    "        fda_ptr = 0\n",
    "        real_ptr = 0\n",
    "        \n",
    "        for _ in range(self.num_batches):\n",
    "            batch_imgs = []\n",
    "            batch_labels = []\n",
    "            \n",
    "            # FDA Samples\n",
    "            for _ in range(self.fda_per_batch):\n",
    "                if fda_ptr >= len(self.fda_indices):\n",
    "                    fda_ptr = 0\n",
    "                    if self.shuffle: random.shuffle(self.fda_indices)\n",
    "                img, label = self.fda_dataset[self.fda_indices[fda_ptr]]\n",
    "                batch_imgs.append(img)\n",
    "                batch_labels.append(label)\n",
    "                fda_ptr += 1\n",
    "            \n",
    "            # Real Samples\n",
    "            for _ in range(self.real_per_batch):\n",
    "                if real_ptr >= len(self.real_indices):\n",
    "                    real_ptr = 0\n",
    "                    if self.shuffle: random.shuffle(self.real_indices)\n",
    "                img, label = self.real_dataset[self.real_indices[real_ptr]]\n",
    "                batch_imgs.append(img)\n",
    "                batch_labels.append(label)\n",
    "                real_ptr += 1\n",
    "                \n",
    "            # Stack\n",
    "            batch_imgs = torch.stack(batch_imgs)\n",
    "            batch_labels = torch.stack(batch_labels)\n",
    "            \n",
    "            # Shuffle batch\n",
    "            perm = torch.randperm(len(batch_imgs))\n",
    "            yield batch_imgs[perm], batch_labels[perm]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "train_loader = HybridDataLoader(ds_pastis, ds_algeria_train, batch_size=CONFIG['batch_size'])\n",
    "test_loader = DataLoader(ds_algeria_test, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "print(f\"\\nFINAL DATA SETUP:\")\n",
    "print(f\"  Training: ~{len(train_loader) * CONFIG['batch_size']} samples (Hybrid)\")\n",
    "print(f\"  Testing:  {len(ds_algeria_test)} samples (Algeria Only)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7192b245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crop/Desktop/crop2/myenv/lib/python3.12/site-packages/timm/layers/interpolate.py:47: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /pytorch/aten/src/ATen/native/BucketizationUtils.h:32.)\n",
      "  idx_right = torch.bucketize(x, p)\n",
      "/home/crop/Desktop/crop2/myenv/lib/python3.12/site-packages/timm/layers/interpolate.py:65: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:347.)\n",
      "  numerator += self.values[as_s] * \\\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: Swin Model Definition\n",
    "# =============================================================================\n",
    "\n",
    "def create_swin_binary():\n",
    "    # Use standard 224 model and adapt\n",
    "    model = timm.create_model(\n",
    "        'swin_tiny_patch4_window7_224', \n",
    "        pretrained=True,\n",
    "        num_classes=1, # Binary\n",
    "        in_chans=3,\n",
    "        img_size=128\n",
    "    )\n",
    "    \n",
    "    # Patch Embedding Hack for 10 Channels\n",
    "    original_proj = model.patch_embed.proj\n",
    "    new_proj = nn.Conv2d(\n",
    "        in_channels=CONFIG['in_channels'],\n",
    "        out_channels=original_proj.out_channels,\n",
    "        kernel_size=original_proj.kernel_size,\n",
    "        stride=original_proj.stride,\n",
    "        padding=original_proj.padding\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        new_proj.weight[:, :3, :, :] = original_proj.weight\n",
    "        mean_weight = torch.mean(original_proj.weight, dim=1, keepdim=True)\n",
    "        new_proj.weight[:, 3:, :, :] = mean_weight.repeat(1, 7, 1, 1)\n",
    "        new_proj.bias.copy_(original_proj.bias)\n",
    "        \n",
    "    model.patch_embed.proj = new_proj\n",
    "    return model\n",
    "\n",
    "model = create_swin_binary().to(device)\n",
    "print(\"Model created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc44d8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss 0.1936 | Val Acc 0.7647\n",
      "  >>> New Best Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss 0.0154 | Val Acc 0.8015\n",
      "  >>> New Best Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss 0.0099 | Val Acc 0.7794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss 0.0041 | Val Acc 0.7794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss 0.0080 | Val Acc 0.7206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss 0.0052 | Val Acc 0.7721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss 0.0026 | Val Acc 0.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss 0.0016 | Val Acc 0.7794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss 0.0017 | Val Acc 0.7721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss 0.0033 | Val Acc 0.7721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss 0.0084 | Val Acc 0.7574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss 0.0036 | Val Acc 0.7941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss 0.0022 | Val Acc 0.7868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss 0.0066 | Val Acc 0.7794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m model.train()\n\u001b[32m     16\u001b[39m total_loss = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/crop2/myenv/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mHybridDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     63\u001b[39m     fda_ptr = \u001b[32m0\u001b[39m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.shuffle: random.shuffle(\u001b[38;5;28mself\u001b[39m.fda_indices)\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m img, label = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfda_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfda_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfda_ptr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     66\u001b[39m batch_imgs.append(img)\n\u001b[32m     67\u001b[39m batch_labels.append(label)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mPastisDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     35\u001b[39m     target_idx = random.randint(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.target_style_images)-\u001b[32m1\u001b[39m)\n\u001b[32m     36\u001b[39m     target_img = \u001b[38;5;28mself\u001b[39m.target_style_images[target_idx]\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     img = \u001b[43mapply_fda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfda_beta\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Label 1 (We treat PASTIS crops as Positive training examples)\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.from_numpy(img), torch.tensor(\u001b[32m1.0\u001b[39m, dtype=torch.float32)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mapply_fda\u001b[39m\u001b[34m(source_img, target_img, beta)\u001b[39m\n\u001b[32m     36\u001b[39m new_amp = np.fft.ifftshift(src_amp_shifted, axes=(-\u001b[32m2\u001b[39m, -\u001b[32m1\u001b[39m))\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# 4. Reconstruct\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m new_fft = new_amp * \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_phase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m new_img = np.abs(np.fft.ifft2(new_fft, axes=(-\u001b[32m2\u001b[39m, -\u001b[32m1\u001b[39m)))\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.clip(new_img, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m).astype(np.float32)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 7: Training Loop\n",
    "# =============================================================================\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10)\n",
    "\n",
    "best_acc = 0.0\n",
    "history = {'train_loss': [], 'val_acc': []}\n",
    "\n",
    "print(\"Starting Training Pipeline...\")\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "    # --- Train ---\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False):\n",
    "        images, labels = images.to(device), labels.to(device).unsqueeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device).unsqueeze(1)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "    val_acc = correct / total\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train Loss {avg_train_loss:.4f} | Val Acc {val_acc:.4f}\")\n",
    "    \n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_binary_crop_model.pth')\n",
    "        print(\"  >>> New Best Model Saved\")\n",
    "        \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a849790f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Full Evaluation on Combined Test Set...\n",
      "\n",
      "==================================================\n",
      "FINAL RESULTS: CROP (Cereal+Potato) vs NO-CROP\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No-Crop       0.83      0.80      0.81        74\n",
      "        Crop       0.77      0.81      0.79        62\n",
      "\n",
      "    accuracy                           0.80       136\n",
      "   macro avg       0.80      0.80      0.80       136\n",
      "weighted avg       0.80      0.80      0.80       136\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: Final Evaluation\n",
    "# =============================================================================\n",
    "\n",
    "# Load Best Model\n",
    "model.load_state_dict(torch.load('best_binary_crop_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "print(\"\\nRunning Full Evaluation on Combined Test Set...\")\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        \n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL RESULTS: CROP (Cereal+Potato) vs NO-CROP\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(y_true, y_pred, target_names=['No-Crop', 'Crop']))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No-Crop', 'Crop'], yticklabels=['No-Crop', 'Crop'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28555feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Optimized Grid Search over 1 configurations...\n",
      "GPUs Available: 2\n",
      "Constraint: Max 8 Epochs per run + Early Stopping.\n",
      "\n",
      "[Run 1/1] Config: {'learning_rate': 1e-05, 'weight_decay': 0.05, 'drop_rate': 0.3, 'fda_beta': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crop/Desktop/crop2/myenv/lib/python3.12/site-packages/timm/layers/interpolate.py:65: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:347.)\n",
      "  numerator += self.values[as_s] * \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Result: Val Acc 0.8603 | Time: 345.8s\n",
      "   🌟 New Global Best! Saving model...\n",
      "\n",
      "==================================================\n",
      "🏆 GRID SEARCH COMPLETE in 5.8 minutes\n",
      "Best Validation Accuracy: 0.8603\n",
      "Best Config: {'learning_rate': 1e-05, 'weight_decay': 0.05, 'drop_rate': 0.3, 'fda_beta': 0.05}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# --- 1. Define Hyperparameter Grid ---\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-5],\n",
    "    'weight_decay': [0.05],\n",
    "    'drop_rate': [0.3],          # Head Dropout\n",
    "    'fda_beta': [0.05]          # Domain Adaptation Strength\n",
    "}\n",
    "\n",
    "# Generate combinations\n",
    "keys, values = zip(*param_grid.items())\n",
    "combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "print(f\"🚀 Starting Optimized Grid Search over {len(combinations)} configurations...\")\n",
    "print(f\"GPUs Available: {torch.cuda.device_count()}\")\n",
    "print(f\"Constraint: Max 8 Epochs per run + Early Stopping.\")\n",
    "\n",
    "# Storage for results\n",
    "results = []\n",
    "best_overall_acc = 0.0\n",
    "best_config = {}\n",
    "\n",
    "start_total = time.time()\n",
    "\n",
    "# --- 2. Grid Search Loop ---\n",
    "for i, config in enumerate(combinations):\n",
    "    run_start = time.time()\n",
    "    print(f\"\\n[Run {i+1}/{len(combinations)}] Config: {config}\")\n",
    "    \n",
    "    # A. Setup Data (Update FDA Beta globally)\n",
    "    CONFIG['fda_beta'] = config['fda_beta'] \n",
    "    \n",
    "    # Re-init Pastis to ensure beta is applied correctly if cached (though mostly dynamic)\n",
    "    # Note: Removed 'transform' argument to match your original class definition\n",
    "    current_ds_pastis = PastisDataset(PATH_PASTIS, target_style_images=style_refs)\n",
    "    grid_train_loader = HybridDataLoader(current_ds_pastis, ds_algeria_train, batch_size=32)\n",
    "    \n",
    "    # B. Initialize Model\n",
    "    model = timm.create_model(\n",
    "        'swin_tiny_patch4_window7_224', \n",
    "        pretrained=True, \n",
    "        num_classes=1, \n",
    "        in_chans=3,\n",
    "        img_size=128,\n",
    "        drop_rate=config['drop_rate']\n",
    "    )\n",
    "    \n",
    "    # 10-Channel Patch Embedding Fix\n",
    "    orig_proj = model.patch_embed.proj\n",
    "    new_proj = nn.Conv2d(10, orig_proj.out_channels, kernel_size=orig_proj.kernel_size, stride=orig_proj.stride, padding=orig_proj.padding)\n",
    "    with torch.no_grad():\n",
    "        new_proj.weight[:, :3] = orig_proj.weight\n",
    "        new_proj.weight[:, 3:] = torch.mean(orig_proj.weight, dim=1, keepdim=True).repeat(1, 7, 1, 1)\n",
    "        new_proj.bias.copy_(orig_proj.bias)\n",
    "    model.patch_embed.proj = new_proj\n",
    "    \n",
    "    # Dual GPU Support\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # C. Optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # D. Fast Training Loop with Early Stopping\n",
    "    MAX_EPOCHS = 8\n",
    "    PATIENCE = 3\n",
    "    best_run_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        # Train\n",
    "        model.train()\n",
    "        for imgs, lbls in grid_train_loader:\n",
    "            imgs, lbls = imgs.to(device), lbls.to(device).unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, lbls)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Validation (Strictly for Model Selection)\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, lbls in test_loader: # using test_loader as validation set here\n",
    "                imgs, lbls = imgs.to(device), lbls.to(device).unsqueeze(1)\n",
    "                outputs = model(imgs)\n",
    "                preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                correct += (preds == lbls).sum().item()\n",
    "                total += lbls.size(0)\n",
    "        \n",
    "        val_acc = correct / total\n",
    "        \n",
    "        # Check Best for this Run\n",
    "        if val_acc > best_run_val_acc:\n",
    "            best_run_val_acc = val_acc\n",
    "            patience_counter = 0 # Reset patience\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        # Early Stopping Trigger\n",
    "        if patience_counter >= PATIENCE:\n",
    "            # print(f\"   -> Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "            \n",
    "    run_time = time.time() - run_start\n",
    "    print(f\"   -> Result: Val Acc {best_run_val_acc:.4f} | Time: {run_time:.1f}s\")\n",
    "    \n",
    "    # Record Result\n",
    "    results.append({'config': config, 'acc': best_run_val_acc})\n",
    "    \n",
    "    # E. Save Global Best\n",
    "    if best_run_val_acc > best_overall_acc:\n",
    "        best_overall_acc = best_run_val_acc\n",
    "        best_config = config\n",
    "        print(f\"   🌟 New Global Best! Saving model...\")\n",
    "        \n",
    "        # Handle DataParallel wrapper for saving\n",
    "        state_dict = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n",
    "        torch.save(state_dict, 'best_grid_search_model.pth')\n",
    "\n",
    "total_time = (time.time() - start_total) / 60\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"🏆 GRID SEARCH COMPLETE in {total_time:.1f} minutes\")\n",
    "print(f\"Best Validation Accuracy: {best_overall_acc:.4f}\")\n",
    "print(f\"Best Config: {best_config}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f009346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model found with config: {'learning_rate': 1e-05, 'weight_decay': 0.05, 'drop_rate': 0.3, 'fda_beta': 0.05}\n",
      "Running Final Evaluation...\n",
      "\n",
      "==================================================\n",
      "WINNER MODEL RESULTS\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No-Crop       0.83      0.85      0.84        74\n",
      "        Crop       0.82      0.79      0.80        62\n",
      "\n",
      "    accuracy                           0.82       136\n",
      "   macro avg       0.82      0.82      0.82       136\n",
      "weighted avg       0.82      0.82      0.82       136\n",
      "\n",
      "Confusion Matrix:\n",
      "[[63 11]\n",
      " [13 49]]\n"
     ]
    }
   ],
   "source": [
    "# --- Load and Evaluate the Champion Model ---\n",
    "print(f\"Loading best model found with config: {best_config}\")\n",
    "\n",
    "# Re-build model structure to match saved weights\n",
    "best_model = timm.create_model(\n",
    "    'swin_tiny_patch4_window7_224', \n",
    "    pretrained=False, # Weights loaded from disk\n",
    "    num_classes=1, \n",
    "    in_chans=10,\n",
    "    img_size=128,\n",
    "    drop_rate=0,\n",
    "    learning_rate=best_config['learning_rate'],\n",
    "    weight_decay=best_config['weight_decay'],\n",
    "    fda_beta = 0.05 \n",
    ")\n",
    "\n",
    "# Fix Input Layer\n",
    "orig_proj = best_model.patch_embed.proj\n",
    "new_proj = nn.Conv2d(10, orig_proj.out_channels, kernel_size=orig_proj.kernel_size, stride=orig_proj.stride, padding=orig_proj.padding)\n",
    "best_model.patch_embed.proj = new_proj\n",
    "\n",
    "# Load Weights\n",
    "best_model.load_state_dict(torch.load('best_grid_search_model.pth'))\n",
    "best_model.to(device)\n",
    "best_model.eval()\n",
    "\n",
    "# Run Inference\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "print(\"Running Final Evaluation...\")\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = best_model(images)\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        \n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"WINNER MODEL RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(y_true, y_pred, target_names=['No-Crop', 'Crop']))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84248eff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
